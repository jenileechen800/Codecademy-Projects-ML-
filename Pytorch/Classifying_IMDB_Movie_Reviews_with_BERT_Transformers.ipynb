{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b58914-fdf1-4423-b405-2d8b8f91bd1b",
   "metadata": {},
   "source": [
    "# Classifying IMDB Movie Reviews with BERT Transformers\n",
    "\n",
    "- [View Solution Notebook](./solutions.html)\n",
    "- [View Project Page](https://www.codecademy.com/content-items/29838c7636654e48ac72458af6373d5d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90f6ef-0659-4afb-8b6e-dc40266b82b1",
   "metadata": {},
   "source": [
    "**Setup - Import Libraries**\n",
    "\n",
    "Run the cell below to import the libraries and set the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275582c5-c2ca-43ae-85f0-5cdf88b36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42) # set random seed --do not change!\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error() # remove warnings --do not change!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5c280-304f-4038-8cb4-1c3c795d5fbd",
   "metadata": {},
   "source": [
    "## Task Group 1 - Import and Inspect the Movie Reviews\n",
    "\n",
    "Let's first import and inspect the datasets containing the movie reviews!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1f258-9aaa-4cc8-9150-99f010d06b88",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "In this project, we'll be classifying different _aspects_ within movie reviews from IMDB (Internet Movie Database), which is a free online database containing data on movies, TV shows, and other types of media.\n",
    "\n",
    "We've obtained the movie reviews from this [Hugging Face Dataset](https://huggingface.co/datasets/Lowerated/lm6-movies-reviews-aspects) that we've cleaned and pre-processed into the following files:\n",
    "- `datasets/imdb_movie_reviews_train.csv` - contains movie reviews in the training dataset\n",
    "- `datasets/imdb_movie_reviews_test.csv` - contains the movie reviews in the testing dataset\n",
    "\n",
    "Here's a quick summary of the columns in the dataset:\n",
    "\n",
    "- `review` - The text of the movie review.\n",
    "- `aspect` - The thematic aspect of the movie the review targets.\n",
    "- `aspect_encoded` An integer label encoding the `aspect` column.\n",
    "\n",
    "Start by importing the CSV file to a pandas DataFrame named `train_reviews_df`.\n",
    "\n",
    "Use the `.head()` method to preview the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7219599-d548-4d36-9a76-73b861f9c5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/imdb_movie_reviews_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_reviews_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/imdb_movie_reviews_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_reviews_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/imdb_movie_reviews_train.csv'"
     ]
    }
   ],
   "source": [
    "train_reviews_df = pd.read_csv('datasets/imdb_movie_reviews_train.csv')\n",
    "train_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f06982-e313-482c-b4ac-f81dd9a27fcd",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Next, use `.info()` to inspect the training dataset. Make sure the data types for each column make sense and if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502b157-10a8-49a8-a83a-2020388fb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369 entries, 0 to 368\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   review          369 non-null    object\n",
      " 1   aspect          369 non-null    object\n",
      " 2   aspect_encoded  369 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8b644-3296-46de-bb91-40a21b133480",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Let's explore the different aspect labels in the training dataset.\n",
    "\n",
    "Use the `.value_counts()` method on the `aspect` column to count the number of aspect labels.\n",
    "\n",
    "Do the same for the `aspect_encoded` column to verify that the number of aspect labels correspond equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a940209-b9f2-4390-be70-fa8c04d4b04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect_encoded\n",
       "0    125\n",
       "1    123\n",
       "2    121\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews_df['aspect'].value_counts()\n",
    "train_reviews_df['aspect_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cb954-2249-4b7f-a4b9-a0d5c5de77db",
   "metadata": {},
   "source": [
    "Here's a quick summary of each aspect:\n",
    "\n",
    "- **Cinematography** focuses on the movie's visual elements like the locations, quality, lighting, and visual appeal.\n",
    "- **Characters** addresses the portrayal of characters and their development throughout the movie, which can focus on their acting, personality, depth, and relatability.\n",
    "- **Story** highlights the movie's themes, plots, originality, and quality of storytelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be883d99-a973-423c-9766-399fb6519604",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Since each movie review is classified into one of the three aspects, we'll frame this task as a **multi-class** classification task. \n",
    "\n",
    "Let's save the number of aspects to the variable `n_aspects`, which we'll reference later when we build the neural network and BERT transformer using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f168ad-fe32-4897-bd84-d9f2fa9a7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aspects = train_reviews_df['aspect'].nunique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20b1cb-527e-4e57-988a-117055232e63",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Next, create the _training corpus_ by using the `.tolist()` method to convert the text sequence in each review and their corresponding aspect labels to the following lists:\n",
    "\n",
    "- `train_texts` contains each movie review separated by commas.\n",
    "- `train_labels` contains each review aspect separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fa105-cdbd-406a-8cdd-961019222cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_reviews_df['review'].tolist()\n",
    "train_labels = train_reviews_df['aspect_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a602e-8f34-4276-9f8d-617a3046e957",
   "metadata": {},
   "source": [
    "## Task Group 2 - Pre-processing the Text Data\n",
    "\n",
    "Next, we'll need to pre-process the text data into a numerical representation that our text classification model will understand!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7779c2-e0e1-4447-af48-c67dd3b58392",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Next, let's tokenize each review into word tokens.\n",
    "\n",
    "A. Create a function named `tokenize_review()` that uses each review text as input.\n",
    "- With the `re` module, use the `findall()` function to lowercase the text using `.lower()` and locate words (without punctuations and special characters) using the regular expression `r'\\b\\w+\\b'`.\n",
    "- Save the tokenized text to the variable `tokenized_review_text` within the function.\n",
    "- Make sure the function returns the tokenized text.\n",
    "\n",
    "B. Apply the function to tokenize each review text in the training corpus.\n",
    "- Use a `for` loop to apply the `tokenize()` function to each review text in `train_texts`.\n",
    "- Save the list of tokenized reviews to the variable `tokenized_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581436b8-1aea-444a-b8db-d4442b5bbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def tokenize_review(review_text):\n",
    "    tokenized_review_text = re.findall(r'\\b\\w+\\b', review_text.lower())\n",
    "    return tokenized_review_text\n",
    "\n",
    "tokenized_corpus = []\n",
    "for text in train_texts:\n",
    "    tokenized_corpus.append(tokenize_review(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102c8eb-5e49-490c-8e5d-b99533b212dd",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "From the `collections` module, use the `Counter()` function to count the number of occurrences of each token and save the counts to the variable `word_freqs`.\n",
    "- First, we'll need to combine each tokenized text sequence together into a single, long list of tokens of the full training corpus. We'll save the list to the variable `combined_corpus`.\n",
    "- Next, you'll need to create a _nested loop_ that first loops through each tokenized text sequence within the training corpus.\n",
    "- Then, the second loop iterates through each token within each text sequence and appends each token to `combined_corpus`.\n",
    "- Lastly, apply the `Counter()` function to `combined_corpus` to count the number of occurrences of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634d392-947d-4afe-bdf8-d32f0b6c3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# flat list of items \n",
    "combined_corpus = []\n",
    "for tokenized_text in tokenized_corpus:\n",
    "    combined_corpus.extend(tokenized_text)\n",
    "\n",
    "word_freqs = Counter(combined_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e3d75-135e-40f9-83c7-8bf8836b2b41",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Use the `.most_common()` method to obtain the top 1000 most commonly occurring tokens.\n",
    "- Set the value `1000` to the variable `MAX_VOCAB_SIZE`.\n",
    "- Save the top 1000 most commonly occurring tokens to the variable `most_common_words`.\n",
    "\n",
    "Print out the top 10 most common words in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7128789-c86d-4b6b-bce7-7973cc522009",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "most_common_words = word_freqs.most_common(MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02877c-4d85-4d3a-b09a-0e025f07609e",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Create the vocabulary as a dictionary using the top 1000 most commonly occurring word tokens.\n",
    "- Use the `enumerate()` function to assign token IDs for each word token in `most_common_words` based on its positional value *starting at the value 2*.\n",
    "- Save the vocabulary to the variable `vocab`.\n",
    "\n",
    "Add the special tokens `<unk>` and `<pad>` to the vocabulary by:\n",
    "- assigning the special token key `<unk>` with the token ID value `0`\n",
    "- assigning the special token key `<pad>` with the token ID value `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03088d1d-7c54-422e-a460-a75692fb4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx + 2 for idx, (word, freq) in enumerate(most_common_words)}\n",
    "\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d69cb5-0770-4c3e-83cd-738498f60fe4",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Using the vocabulary, create a function named `encode_text()` that tokenizes and encodes each review text into a sequence of token IDs.\n",
    "- Specify the inputs `text` and `vocab`.\n",
    "    - `text` will be the raw review text to be tokenized and encoded.\n",
    "    - `vocab` is the vocabulary.\n",
    "- Apply the `tokenize()` function to the input `text`.\n",
    "- Encode the tokenized text into a sequence of token IDs.\n",
    "    - Assign the token ID value for the special token `<unk>` for word tokens that are not in the vocabulary.\n",
    "- The function should return the encoded review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d54afa-cf5f-4608-9a9d-c00acdcfa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab):\n",
    "    tokenized_text = tokenize_review(text)\n",
    "    encoded_text = [vocab.get(word, vocab['<unk>']) for word in tokenized_text]\n",
    "    return encoded_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8759be0-71e0-408a-8aec-62d5e813d81b",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Let's create another function named `pad_or_truncate()` that pre-processes each encoded text to have the same length specified by a maximum length value.\n",
    "- Specify the inputs `encoded_text` and `max_len`.\n",
    "    - `encoded_text` will be the input encoded review text.\n",
    "    - `max_len` is the specified maximum length value.\n",
    "- Use an `if` statement to identify reviews longer than the maximum length and returns the review truncated to the maximum length value.\n",
    "- Use an `else` statement to identify reviews shorter than the maximum length and returns the review padded with `1` values (corresponding to the token ID for the special token `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3bfec-6d34-4220-82ca-202af4028256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(encoded_text, max_len):\n",
    "    if (len(encoded_text) > max_len):\n",
    "        return encoded_text[:max_len]\n",
    "    else:\n",
    "        return encoded_text + [1] * (max_len - len(encoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7720614-300f-4608-a067-0a26faaf49a9",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Now, let's fully tokenize and encode each review text by applying the `encode_text()` and `pad_or_truncate()` functions:\n",
    "- Specify a maximum length of `128` for each review text sequence and save the value to the variable `MAX_SEQ_LENGTH`.\n",
    "- Tokenize and encode each review text as follows:\n",
    "    - Use a `for` loop to iterate through each training review text in `train_texts`.\n",
    "    - Apply the `encode_text()` function to each review text (be sure to specify the vocabulary).\n",
    "    - Apply the `pad_or_truncate()` function to each encoded text (be sure to specify the maximum length value).\n",
    "- Save the fully pre-processed review text sequences to the variable `padded_text_seqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5c039-bdbb-494f-86a5-8947e7ee84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "padded_text_seqs = []\n",
    "\n",
    "for text in train_texts:\n",
    "    encoded_text = encode_text(text, vocab)\n",
    "    correct_length_text = pad_or_truncate(encoded_text, MAX_SEQ_LENGTH)\n",
    "    padded_text_seqs.append(correct_length_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b242c14-bbb0-49a2-a336-104601566aa1",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "Let's prepare our text sequences to for training by converting them into PyTorch tensors:\n",
    "\n",
    "- Create the input tensor `X_tensor` by converting the padded/truncated sequences.\n",
    "- Create the label tensor `y_tensor` by converting the training review labels (be sure to specify the datatype `torch.long`).\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- Create the variable `train_dataset` using the `TensorDataset` utility class to organize the input tensor and label tensor into a single dataset object.\n",
    "- Create the variable `train_dataloader` using the `DataLoader` utility class to create an iterable that loads the `train_dataset` in batches of `16` (be sure to set `shuffle` to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcb32d-e5da-4522-b143-93f4c31a0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_tensor = torch.tensor(padded_text_seqs)\n",
    "y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10404cf-c27f-412f-aca9-c7f3a4ac813a",
   "metadata": {},
   "source": [
    "## Task Group 3 - Training a Simple Neural Network\n",
    "\n",
    "The first text classification model we'll build and train to classify movie reviews is a simple neural network with an embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f761e-513c-4993-a57d-42f82f6e02c5",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "Let's construct the neural network architecture that will be trained to classify movie reviews!\n",
    "\n",
    "Create a class named `SimpleNNWithEmbedding` for our neural network (using the PyTorch's `nn.Module`) with the following:\n",
    "\n",
    "A. Create the `.init()` method that takes in the following attributes:\n",
    "- `vocab_size` is the number of tokens in the vocabulary. \n",
    "- `embed_size` is the embedding size.\n",
    "- `hidden_size` is the number of neurons in the linear layer.\n",
    "- `output_size` is the number of output classes.\n",
    "\n",
    "B. Build the `.init()` method by initializing the following layers:\n",
    "- `self.embedding` is an embedding layer that creates embeddings equal to the vocabulary size with embedding sizes specified by the `embed_size` input.\n",
    "- `self.fc1` is the first linear layer with an input size equal to the embedding size in the embedding layer and an output size equal to the number of neurons in the hidden layer specified by the `hidden_size` input.\n",
    "- `self.fc2` is the second linear layer with an input size equal to the hidden size of the first linear layer and an output size equal to the number of classes specified by the `output_size` input.\n",
    "\n",
    "C. Build the `.forward()` method and create the forward operations in the following order:\n",
    "1. Start by passing the input `x` into the embedding layer.\n",
    "2. Average the embeddings into a single representation.\n",
    "3. Pass the averaged embedding into the first linear layer.\n",
    "4. Apply the ReLU activation function.\n",
    "5. Pass the activated output to the second linear layer.\n",
    "6. Return the output of the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5557d49-7af6-4999-bdc2-e7d41b31d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SimpleNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a4d0b-ebe2-48e3-afc9-1586229d93e8",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "\n",
    "Next, let's create an instance of our `SimpleNNWithEmbedding` neural network class.\n",
    "\n",
    "Start by defining the following variables:\n",
    "- `vocab_size` - integer containing the vocabulary size (vocabulary length)\n",
    "- `embed_size` - an embedding size of `50` dimensions for each token\n",
    "- `hidden_size` - specifies the hidden layer in the neural network with `100` neurons\n",
    "- `output_size` - specifies the number of class labels in our **multi-class** classification task\n",
    "    - Hint: Each review will be classified as one of the following aspects: Cinematography, Characters, or Story\n",
    " \n",
    "Instantiate the model with the variable parameters to the variable `text_classifier_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1e1a7-7232-4301-a5f8-6e16459e0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 50\n",
    "hidden_size = 100\n",
    "output_size = 3\n",
    "\n",
    "text_classifier_nn = SimpleNNWithEmbedding(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8df372-41f0-4fdb-9ebe-f977a32ec942",
   "metadata": {},
   "source": [
    "### Task 16\n",
    "\n",
    "Next, let's initialize the loss function and optimizer for training:\n",
    "- Create an instance of the cross-entropy loss function in PyTorch (from the `torch.nn` module) and save it to the variable `criterion`.\n",
    "- Create an instance of the Adam optimizer in PyTorch (from the `torch.optim` module) with a learning rate of `0.005` and save it to the variable `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1926e79-6ac8-43a6-a0e5-e63315b09657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_classifier_nn.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4ff2c-5dd1-495b-982a-d3ade22b8d18",
   "metadata": {},
   "source": [
    "### Task 17\n",
    "\n",
    "Now let's train our neural network to classify movie reviews!\n",
    "\n",
    "A. Create a function named `train_model()` that takes in the following inputs:\n",
    "- `model` is the text classification model.\n",
    "- `train_loader` is the training data contained in a PyTorch `DataLoader` object.\n",
    "- `criterion` is the loss function used to train the model.\n",
    "- `optimizer` is the optimizer used to train the model.\n",
    "- `num_epochs` is the number of training epochs.\n",
    "\n",
    "B. In the function, create a `for` loop that loops through the number of epochs specified by the input `num_epochs`. Within the loop:\n",
    "- Set the model to training mode.\n",
    "- Track the loss per epoch by initializing the variable `epoch_loss` to `0.0`.\n",
    "\n",
    "C. Create a nested `for` loop within the first `for` loop such that:\n",
    "\n",
    "1. Loops through the inputs and labels of each training batch in the input `train_loader`. \n",
    "2. Reset the gradients at each iteration.\n",
    "3. Input the training batch through the forward pass.\n",
    "4. Calculate the cross-entropy loss.\n",
    "5. Backpropagate the loss through the network.\n",
    "6. Adjust the weights and biases.\n",
    "7. Update the total loss within the current training epoch.\n",
    "\n",
    "D. Run the function to train the `text_classifier_nn` model on the training dataset stored in the iterable `train_dataloader` using the loss-function saved in `criterion` and the optimizer saved in `optimizer` for `50` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3bea0-ab65-468d-b968-5d3fcf8f6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in num_epochs:\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "train_model(text_classifier_nn, train_dataloader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8b196-6de4-4d6d-9257-88adb9bb9a94",
   "metadata": {},
   "source": [
    "### Task 18\n",
    "\n",
    "Let's evaluate the trained neural network on an out-of-sample testing dataset.\n",
    "\n",
    "Import the testing dataset stored in the CSV file `datasets/imdb_movie_reviews_test.csv` to a pandas DataFrame named `test_reviews_df`.\n",
    "\n",
    "Then, create the _testing corpus_ by using the `.tolist()` method to convert the text sequence in each review and their corresponding aspect labels to the following lists:\n",
    "\n",
    "- `test_texts` contains each movie review separated by commas.\n",
    "- `test_labels` contains each review aspect separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61ab47-9a7a-41f2-99f4-4e513a46e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews_df = pd.read_csv('datasets/imdb_movie_reviews_test.csv')\n",
    "\n",
    "test_texts = test_reviews_df['review'].tolist()\n",
    "test_labels = test_reviews_df['aspect_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590de0b-e30e-48b1-878d-541fe518cb0b",
   "metadata": {},
   "source": [
    "### Task 19\n",
    "\n",
    "Let's pre-process (tokenize, encode, pad, and truncate) the text sequences in the testing dataset using the previous functions:\n",
    "- `encode_text` - uses the vocabulary to tokenize and encode text into token IDs\n",
    "- `pad_or_truncate` - pads or truncates the encoded text to a specified maximum sequence length\n",
    "\n",
    "Be sure to use the same maximum sequence length we used for the training dataset.\n",
    "\n",
    "Save the pre-processed test sequences to the variable `padded_text_seqs_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692fdec-1946-412d-907f-93176f3fd500",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m padded_text_seqs_test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[1;32m      4\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m encode_text(text, vocab)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "padded_text_seqs_test = []\n",
    "\n",
    "for text in test_texts:\n",
    "    encoded_text = encode_text(text, vocab)\n",
    "    correct_length_text = pad_or_truncate(encoded_text, MAX_SEQ_LENGTH)\n",
    "    padded_text_seqs_test.append(correct_length_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5996fb-3d53-4cb7-b6f9-589af49e55eb",
   "metadata": {},
   "source": [
    "### Task 20\n",
    "\n",
    "Next, let's convert the pre-processed test sequences into PyTorch tensors:\n",
    "- Create the input tensor `X_tensor_test` by converting the padded/truncated test sequences.\n",
    "- Create the label tensor `y_tensor_test` by converting the testing labels (be sure to specify thet datatype `torch.long`.\n",
    "\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the testing input and label tensors into a single dataset object and an iterable that will allow us to load the testing data in batches:\n",
    "- Create the variable `test_dataset` using the `TensorDataset` utility class to organize the input tensor and label tensor into a single dataset object.\n",
    "- Create the variable `test_dataloader` using the `DataLoader` utility class to create an iterable that loads the `test_dataset` in batches of `8` (be sure to set `shuffle` to `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c132a-ce3a-4309-80e3-26ef315bb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_test = torch.tensor(padded_text_seqs_test)\n",
    "y_tensor_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4021084-f5da-46fc-9367-0a03e0138358",
   "metadata": {},
   "source": [
    "### Task 21\n",
    "\n",
    "Let's generate predictions from the trained neural network on the reviews in the testing dataset!\n",
    "\n",
    "A. Create the function `get_predictions_and_probabilities()` that takes in the trained model and testing dataloader as input.\n",
    "\n",
    "B. Within the function:\n",
    "- Set the model to evaluation mode.\n",
    "- Initialize the following empty lists:\n",
    "    - `all_probs` - Stores all of the predicted probabilities for the testing dataset.\n",
    "    - `all_labels` - Stores all of the predicted labels for the testing dataset.\n",
    "\n",
    "- Using `with torch.no_grad()`, loop through each batch in the testing dataloader and:\n",
    "    -  Generate outputs from the forward pass.\n",
    "    -  Use the `softmax()` function to generate predicted probabilities (be sure to add the probabilities to the `all_probs` list using `extend()`.\n",
    "    -  Use the `argmax()` function to select the class label with the highest probabilities (be sure to add the labels to the `all_labels` list using `extend()`.\n",
    "- The function should return the predicted probabilities `all_probs` and predicted labels `all_labels`.\n",
    "\n",
    "C. Apply the `get_predictions_and_probabilities()` function to generate predictions. Save the predicted probabilities to the variable `pred_probs` and predicted labels to the variable `pred_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3fcaa-7c53-40df-b801-78469db1e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_predictions_and_probabilities(model, test_loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            # Apply the forward pass to generate predicted outputs\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Apply softmax to get predicted probabilities\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            # Apply argmax to get predicted labels\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            all_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    return all_probs, all_labels\n",
    "\n",
    "pred_probs, pred_labels = get_predictions_and_probabilities(text_classifier_nn, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7edf8-386a-425c-af8f-b2c36669d223",
   "metadata": {},
   "source": [
    "### Task 22\n",
    "\n",
    "Let's evaluate the model's predictions (`pred_labels`) with the true labels (`test_labels`).\n",
    "\n",
    "A. Generate a confusion matrix to count the number of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). Save the confusion matrix to the variable `conf_matrix`.\n",
    "\n",
    "B. Generate a classification report to calculate the accuracy, precision, recall, and F1-score metrics for each label. Save the classification report to the variable `report`.\n",
    "\n",
    "Print the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce5406-f843-40c3-9078-3e4e32c71955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "\n",
    "print(f\"Confusion Matrix: {conf_matrix}\")\n",
    "print(f\"Classification Report: {report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ed6e0-0ba9-44df-9d41-2eb9788b7655",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How well does the simple neural network perform when classifying movie reviews?</summary>\n",
    "\n",
    "    \n",
    "**Confusion Matrix**\n",
    "\n",
    "We can interpret the confusion matrix by this structure:\n",
    "    \n",
    "```py\n",
    "    [[TP0 FN0  FN0]\n",
    "     [FN1 TP1  FN1]\n",
    "     [FN2 FN2  TP2]]\n",
    "```\n",
    "\n",
    "Each row corresponds to the TPs and FNs for each review aspect: \n",
    "    \n",
    "- row 1 corresponds to the TPs and FNs for Cinematography reviews (label 0)\n",
    "    - 27 Cinematography reviews were correctly classified as Cinematography reviews (label 0)\n",
    "    - 18 Cinematography reviews were incorrectly classified as Character reviews (label 1)\n",
    "    - 4 Cinematography reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 2 corresponds to the TPs and FNs for Character reviews (label 1)\n",
    "    - 35 Character reviews were correctly classified as Character reviews (label 1)\n",
    "    - 1 Character review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 2 Character reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 3 corresponds to the TPs and FNs for Story reviews (label 2)\n",
    "    - 28 Story reviews were correctly classified as Story reviews (label 2)\n",
    "    - 2 Story review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 15 Story reviews were incorrectly classified as Character reviews (label 1)    \n",
    "\n",
    "**Classification Report**\n",
    "\n",
    "When classifying movie reviews, the simple neural network (with an embedding layer) had an overall accuracy of 68%, which is decent but not too great. \n",
    "    \n",
    "For Cinematography reviews (label 0):\n",
    "- the model has a high precision score of 90%, meaning that when it classified a review as a Cinematography review, it was right 90% of the time\n",
    "- however, the model has a low recall score of 55%, meaning that it could only identify 55% of all Cinematography reviews, which suggests that it is overpredicting \n",
    "    \n",
    "For Character reviews (label 1):\n",
    "- the model has a low precision score of 51%, meaning that when it classified a review as a Character review, it was right 51% of the time (pretty much a 50/50 coin flip)\n",
    "- however, the model has a high recall score of 92%, meaning that it successfully identified 92% of all Character reviews\n",
    "- this suggests that the model rarely misses a Character review, but seems to misclassify the other review aspects often\n",
    "    \n",
    "For Story reviews (label 2):\n",
    "- the model has a decent precision score of 82%, meaning that when it classified a review as a Story review, it was right 82% of the time\n",
    "- however, the model has a low recall score of 62%, meaning that it could only identify 62% of all Story reviews\n",
    "    \n",
    "Overall, the model performs moderately well in classifying movie review aspects, but there is definitely room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abae6-9de7-42b4-8c8b-5c0df23e84a6",
   "metadata": {},
   "source": [
    "## Task Group - Fine-tuning a TinyBERT Transformer\n",
    "\n",
    "Let's now train a more advanced BERT transformer language model! Specifically, let's fine-tune a TinyBERT model to classify movie reviews! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837afff7-9f73-4e2a-b538-4cf51b338805",
   "metadata": {},
   "source": [
    "### Task 23\n",
    "\n",
    "Load the pre-trained BERT model using the `transformers` library from Hugging Face.\n",
    "\n",
    "Due to memory and hardware constraints, we won't be able to use the full pre-trained BERT model that contains 12 layers and 110 million parameters. Instead, we'll use a distilled, smaller version of BERT called TinyBERT that contains 4 layers and 14.5 million parameters developed by Huawei Noah's Ark Lab (https://huggingface.co/huawei-noah).\n",
    "\n",
    "A. First, save the TinyBERT model name `'huawei-noah/TinyBERT_General_4L_312D'` as a string to the variable `model_name`.\n",
    "\n",
    "B. Download and load the TinyBERT tokenizer from the `BertTokenizer` module and save the tokenizer to the variable `tinybert_tokenizer`.\n",
    "\n",
    "C. Download and load the TinyBERT models (weights) with the `BertForSequenceClassification` module. Be sure to specify `num_labels=` with the correct number of class labels (# of movie aspects). Save the TinyBERT model to the variable `text_classifier_tinybert`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1b125-2c19-4b9a-b5fb-84166724daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "text_classifier_tinybert = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b100d-a34d-4ecf-91d4-f3318c3255e8",
   "metadata": {},
   "source": [
    "### Task 24\n",
    "\n",
    "Before fine-tuning the TinyBERT, let's *freeze* and *unfreeze* the following layers:\n",
    "\n",
    "- First, freeze all of the parameters in the pre-trained TinyBERT.\n",
    "- Second, unfreeze the classification layer added on top of the pre-trained model.\n",
    "- Third, unfreeze the encoder layer specified at `layer[3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882aba4-632b-4043-bd85-47e13e4e47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the pre-trained layers\n",
    "for param in text_classifier_tinybert.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the feed-forward classification layer\n",
    "for param in text_classifier_tinybert.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze an encoder layer\n",
    "for param in text_classifier_tinybert.bert.encoder.layer[3].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3823a4-d934-411b-b375-63fcd4f7c6be",
   "metadata": {},
   "source": [
    "### Task 25\n",
    "\n",
    "Let's pre-process the training text reviews (`train_texts`) with the TinyBERT tokenizer:\n",
    "- Specify a maximum sequence length of `124` and save the value to the variable `MAX_SEQ_LENGTH_TINYBERT`.\n",
    "- Create the tensor `X_train` that contains the tokenized training text tokenized by the TinyBERT tokenizer:\n",
    "    - The sequences have a maximum length of 128 tokens.\n",
    "    - The sequences are padded.\n",
    "    - The sequences are truncated.\n",
    "    - The padded and truncated sequences are returned as PyTorch tensors.\n",
    "- Create the tensor `y_train` that contains the corresponding training labels in `train_labels` converted to a PyTorch tensor (be sure to specify the `torch.long` datatype).\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- Create the variable `train_dataset` using the `TensorDataset` utility class to organize the input tensor `X_train` and label tensor `y_train` into a single dataset object (be sure to specify the attention mask in the input tensor created by the TinyBERT tokenizer).\n",
    "- Create the variable `train_dataloader` using the `DataLoader` utility class to create an iterable that loads the `train_dataset` in batches of `16` (be sure to set `shuffle` to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7004e87-7a64-4507-8da5-6f914388a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH_TINYBERT = 124\n",
    "X_train = tinybert_tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train['input_ids'], X_train['attention_mask'], y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd42e2-dbdc-4d9f-843d-4426a96ce30a",
   "metadata": {},
   "source": [
    "### Task 26\n",
    "\n",
    "Next, let's initialize the optimizer and loss function to fine-tune our TinyBERT model (`text_classifier_tinybert`).\n",
    "\n",
    "A. Initialize the Adam optimizer with *weight decay* with a learning rate of `0.0025`. Ensure that only the unfrozen layers are optimized during training. Save the optimizer to the variable `optimizer`.\n",
    "\n",
    "B. Initialize the cross-entropy loss function. Save the loss function to the variable `criterion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d9821-1780-4d4c-82d6-1d3edb48fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, text_classifier_tinybert.parameters()), lr=0.0025)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4dd75-1371-4757-bf65-79b350221a6c",
   "metadata": {},
   "source": [
    "### Task 27\n",
    "\n",
    "Let's create a training loop to fine-tune the TinyBERT model for `10` epochs!\n",
    "\n",
    "A. Set the value of `10` to the variable `num_epochs`.\n",
    "\n",
    "B. Create a training loop first loops through each epoch where in each epoch:\n",
    "- Set the TinyBERT model to training mode.\n",
    "- Initialize the empty variable `total_loss` with `0.0` to keep track of the total loss per epoch.\n",
    "- Create a nested loop that loops through each batch (be sure to specify the batch's attention mask) in the training dataloader where for each batch:\n",
    "    1. Reset the gradients.\n",
    "    2. Apply the forward pass to the training batch (be sure to pass the current batch's attention mask).\n",
    "    3. Extract the logits from the forward pass output.\n",
    "    4. Calculate the cross-entropy loss.\n",
    "    5. Update the `total_loss`.\n",
    "    6. Backpropagate the loss through the network.\n",
    "    7. Adjust the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737017b-f254-4da6-a291-cd0ac2685be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    text_classifier_tinybert.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_attention_mask, batch_y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = text_classifier_tinybert(input_ids= batch_X, attention_mask= batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ee652-d91c-464d-ae02-4adc1a22554e",
   "metadata": {},
   "source": [
    "### Task 28\n",
    "\n",
    "Now that the TinyBERT model is fine-tuned, let's evaluate how the transformer performs on the movie reviews in the testing set and compare its performance with the simple neural network from before!\n",
    "\n",
    "Pre-process the testing dataset into the following PyTorch tensors:\n",
    "\n",
    "- Create the tensor `X_test` using the TinyBERT tokenizer to tokenize and testing dataset and save the pre-processed tensor to the variable `X_test`:\n",
    "    - The maximum sequence length should be same as the length used during fine-tuning the TinyBERT.\n",
    "    - Apply padding and truncating.\n",
    "    - Return the tokenized text as a PyTorch tensor.\n",
    "- Create the tensor `y_test` that contains the testing labels `test_labels` as a PyTorch tensor (be sure to specify the `torch.long` datatype).\n",
    "\n",
    "Using the PyTorch utility module `torch.utils.data`, let's organize the training input and label tensors into a single dataset object and an iterable that will allow us to load the training data in batches:\n",
    "- Create the variable `test_dataset` using the `TensorDataset` utility class to organize the input tensor `X_test` and label tensor `y_test` into a single dataset object (be sure to specify the attention mask in the input tensor created by the TinyBERT tokenizer).\n",
    "- Create the variable `test_dataloader` using the `DataLoader` utility class to create an iterable that loads the `test_dataset` in batches of `8` (be sure to set `shuffle` to `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7403a5-7c72-47dc-9444-2314cecf16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tinybert_tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test['input_ids'], X_test['attention_mask'], y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31d98a-b28a-46e2-9b6b-b52559e8f58d",
   "metadata": {},
   "source": [
    "### Task 29\n",
    "\n",
    "Next, let's generate predictions from the fine-tuned TinyBERT!\n",
    "\n",
    "A. Set the fine-tuned TinyBERT model to evaluation mode.\n",
    "\n",
    "B. Initialize the following empty lists:\n",
    "- `pred_probs` - stores the predicted probabilities for the reviews in the testing dataset\n",
    "- `pred_labels` - stores the predicted labels for the reviews in the testing dataset\n",
    "\n",
    "C. Within `with torch.no_grad()`:\n",
    "- Loop through each batch in the testing dataloader (be sure to specify the attention masks).\n",
    "- Apply the forward pass to each batch.\n",
    "- Obtain the logits from the outputs from the forward pass.\n",
    "- Use the `softmax()` function to generate predicted probabilities (be sure to add the probabilities to the `pred_probs` list using `extend()`.\n",
    "- Use the `argmax()` function to select the class label with the highest probabilities (be sure to add the labels to the `pred_labels` list using `extend()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a679d-ab59-4632-a7db-5f746955e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier_tinybert.eval()\n",
    "\n",
    "pred_probs = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_attention_mask, batch_y in test_dataloader:\n",
    "        # Apply the forward pass\n",
    "        outputs = text_classifier_tinybert(input_ids=batch_X,\n",
    "                                      attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Obtain predicted probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # Obtain predicted labels\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        pred_labels.extend(predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48957b-4d22-40da-8a5f-921fe3a5fc7c",
   "metadata": {},
   "source": [
    "### Task 30\n",
    "\n",
    "Let's see how well our fine-tuned TinyBERT model classifies movie reviews!\n",
    "\n",
    "Evaluate the TinyBERT model's predictions (`pred_labels`) with the true labels (`test_labels`) with the following:\n",
    "\n",
    "A. Generate a confusion matrix to count the number of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). Save the confusion matrix to the variable `conf_matrix`.\n",
    "\n",
    "B. Generate a classification report to calculate the accuracy, precision, recall, and F1-score metrics for each label. Save the classification report to the variable `report`.\n",
    "\n",
    "Print the confusion matrix and classification report.\n",
    "\n",
    "How does the fine-tuned TinyBERT model performance compare to the simple neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb920469-ad9c-4010-ade7-f46a201aaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "report = classification_report(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a972f76-4413-477c-b58b-9e55458ec412",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">How does the fine-tuned TinyBERT model performance compare to the simple neural network?</summary>\n",
    "   \n",
    "**Confusion Matrix**\n",
    "\n",
    "We can interpret the confusion matrix by this structure:\n",
    "    \n",
    "```py\n",
    "    [[TP0 FN0  FN0]\n",
    "     [FN1 TP1  FN1]\n",
    "     [FN2 FN2  TP2]]\n",
    "```\n",
    "\n",
    "Each row corresponds to the TPs and FNs for each review aspect: \n",
    "    \n",
    "- row 1 corresponds to the TPs and FNs for Cinematography reviews (label 0)\n",
    "    - 49 Cinematography reviews were correctly classified as Cinematography reviews (label 0)\n",
    "    - 0 Cinematography reviews were incorrectly classified as Character reviews (label 1)\n",
    "    - 0 Cinematography reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 2 corresponds to the TPs and FNs for Character reviews (label 1)\n",
    "    - 38 Character reviews were correctly classified as Character reviews (label 1)\n",
    "    - 0 Character review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 0 Character reviews were incorrectly classified as Story reviews (label 2)\n",
    "- row 3 corresponds to the TPs and FNs for Story reviews (label 2)\n",
    "    - 35 Story reviews were correctly classified as Story reviews (label 2)\n",
    "    - 8 Story review was incorrectly classified as a Cinematography review (label 0)\n",
    "    - 2 Story reviews were incorrectly classified as Character reviews (label 1)    \n",
    "\n",
    "**Classification Report**\n",
    "\n",
    "When classifying movie reviews, the fine-tuned TinyBERT achieved an overall accuracy of 92%, which outperformed the simple neural network that had an overall accuracy of 68%. \n",
    "    \n",
    "For Cinematography reviews (label 0):\n",
    "- the model has a high precision score of 86%, meaning that when it classified a review as a Cinematography review, it was right 86% of the time\n",
    "- the model has a perfect recall score of 100%, meaning that it correctly identified all Cinematography reviews\n",
    "    \n",
    "For Character reviews (label 1):\n",
    "- the model has a high precision score of 95%, meaning that when it classified a review as a Character review, it was right 95% of the time\n",
    "- the model has a perfect recall score of 100%, meaning that it correctly identified all Character reviews\n",
    "    \n",
    "For Story reviews (label 2):\n",
    "- the model has a perfect precision score of 100%, meaning that when it classified a review as a Story review, it was right 100% of the time\n",
    "- the model has a decent recall score of 78%, meaning that it identified 78% of all Story reviews\n",
    "\n",
    "    \n",
    "The macro and weighted average F1 Score of 92% tell us that the model performs well across all label aspects.\n",
    "    \n",
    "Overall, the model performs exceptionally well in classifying movie review aspects, confirming that fine-tuning a TinyBERT model outperforms a simple neural network for this text classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1caafd",
   "metadata": {},
   "source": [
    "Note: The Jupyter Notebook kernel may crash or be unresponsive after fine-tuning the TinyBERT model. Be sure to save your work every so often to avoid losing any work you've done so far! Feel free to restart the kernel and re-run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a43fd2-880d-4b00-a509-1f555501da06",
   "metadata": {},
   "source": [
    "That's the end of our project on building text classification models to classify different aspects of movie reviews! There's definitely a lot of room for improvement and we encourage you to use your skills to explore different techniques to enhance the language models.\n",
    "\n",
    "Here are some areas for improvement:\n",
    "- increase the maximum sequence length\n",
    "- increase the training batch sizes\n",
    "- freeze/unfreeze different layers in the TinyBERT architecture\n",
    "- increase the number of training epochs\n",
    "- try different optimizers and learning rates\n",
    "- try a different transformer model\n",
    "- increase the size of training dataset ([Full Hugging Face Dataset](https://huggingface.co/datasets/Lowerated/lm6-movies-reviews-aspects))\n",
    "\n",
    "You might want to consider building and training larger language models on your own device or cloud platform with greater memory.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f550e50-f18a-4f7e-9a1d-3be542ed610d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
